{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Exploration\n",
    "\n",
    "    I started with scholar.google.com as the search engine where I would get all my data. It was the only free resource avalilable that had the full texts of judical opinions. I found the site looking for texts of SCOTUS(Supreme Court Of The United States) opinions. However, Google Scholar also had all the 2nd circuit opinions as well, published and unpublished. Thus, i decided to continue workig on my analysis of published v. unpublished opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Success\n",
    "\n",
    "    I was initialy able to webscrape a judical opinion and parse out just the text into a long string. I was also able to parse through search results (narrowed to one year 2012, at 2,110 cases) and parse out the name of the case, the url of the locatoon of the text and if it was unpublished or published(Published citations begin with at number, Unpublished begin with a letter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_scholar_case_logger(search_url, n_pages=1):\n",
    "    \"\"\"Web scrapes google scholar for cases, needs just the /scholar? question link.\n",
    "    returns a DataFrame, but should write the dataframe to a .csv as well.\"\"\"\n",
    "    \n",
    "    google = 'https://scholar.google.com'\n",
    "    final_features_list = []\n",
    "    y = []\n",
    "    for _ in range(0, n_pages):\n",
    "        print 'Page', _\n",
    "        sleep(10)\n",
    "        r = requests.get(google+search_url)\n",
    "        html = r.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        final_feature = []\n",
    "        for el in soup.findAll('h3'):\n",
    "            sleep(30)\n",
    "            print 'Building Case Row'\n",
    "            \n",
    "            sub_r = requests.get(google+el.a['href']) #link to the case page\n",
    "            sub_html = sub_r.text\n",
    "            sub_soup = BeautifulSoup(sub_html, 'lxml')\n",
    "            raw_text = ''\n",
    "            for x in sub_soup.findAll('p'):\n",
    "                raw_text = raw_text + x.get_text() #puts all the text in one long string, raw_text\n",
    "            \n",
    "            final_feature.append([el.get_text(), el.a['href'], raw_text])\n",
    "        for i in soup.findAll('div', {'class':'gs_a'}): #is the citation for the case\n",
    "            try:\n",
    "                int(i.get_text()[0]) #if the citation starts with a number, it is published\n",
    "                y.append(0)\n",
    "            except:\n",
    "                y.append(1)\n",
    "        \n",
    "        final_features_list.append(final_feature)\n",
    "        search_url = soup.find('div', {'id':'gs_n'}).a['href'] # the next button on the search page, links to the next page\n",
    "    \n",
    "    df = pd.DataFrame(final_features_list, columns=['case_name', 'case_url', 'case_text'])\n",
    "    df['Unpublished'] = y\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Obstacle\n",
    "\n",
    "    Before I cound unite the two webscrapes togeather (I wanted a DataFrame that had [name of the case, case url, unpublished(binary), case text] as the columns), google temporarility blocked my ip requests for webscraping (but not my chrome requests). I am thus unable to obtain my entire sample at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Forward\n",
    "\n",
    "    The First thing that must be finished is the code to webscrape goodgle scholar to get the final DataFrame. I need to implement a beter way to scrape google so as to minimize disturbing them. I looked at the terms of service: https://www.google.com/policies/terms/ but I could not find any details on the best way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After obtaining the entire sample\n",
    "\n",
    "    After having sampiled enough texts, I intend to do a basic nlp analysis, and select feaures that would distingiish published from unpublished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forseable Problems\n",
    "\n",
    "    I'm not sure that google scholar has all the unpublished opinions. I looked at the us court records (which publish on sept. 30th) and looked at search results (which is by Jan-Dec of any given year), and the numbers don't totaly match up. will need to look deeper to see if it is just the date causing the numbers not to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
